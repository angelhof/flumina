** Work TODO

*** TODO Setup all the generators for the debs smart home experiments

  - [ ] Implement the hour-window generators
  - [X] Split the big file into house files without heartbeats
  - [X] Add heartbeats to the files of each house
    + [X] In order to do this I have to implement a function that interleaves
          generators with heartbeats
    + [X] I have to extend generate_heartbeat to generate all heartbeats up
          to a point, and not only the last one.
    + [X] I have to extend interleave_heartbeats to handle many messages from generate
          heartbeats.
  - [ ] Implement a setup that doesn't copy the hearbeat files, 
        but gives access to the house files to all docker containers,
        and then each one reads from its own file
  - [ ] Implement an erlang function that gets the latest and first timestamp 
        of the file, or find by hand the latest and earliest timestamp of each 
        file and add it.

*** TODO Start Erlang nodes programmaticaly using the slave module

Instead of starting the erlang nodes when starting the docker
containers, we can start the nodes programmatically, from inside
Erlang, by using the slave module. This way, we will be sure that
Erlang nodes have started, when the experiment starts running.

*** TODO Make the abexample use a tags and not a1 a2 now that we have correct spec/impl

This would actually make some experiments go wrong 
(because we wouldn't be able to setup different a's arriving on the same node)
In a sense, addind key to the tags allows us to create artificial implementation tags to
experiment.

*** TODO Allow the configuration generator to search for a sequence of splits to satisfy a split

*** TODO Clean up calls to functions in mailbox and node to use their state records

Clean up all calls to functions in the mailbox and node to get the mailbox state if they need
big part of its components
  
*** TODO Don't return redundant setup trees (Completeness of similarity relation)

This can be solved by finding the largest root tree similarity relation
so that only one root tree in the similarity class is checked if it matches.

This will let us scale in examples of the form b -> (a1, a2, a3, ...) where
until now our algorithm considers all of a1, a2, a3, ... as possible root tree
matches, when clearly if one matches, the other will match too, and there will 
be no difference whichever we match.

*** TODO Improve the complete root_tree_to_setup_tree
    
I WORRY THAT THIS MIGHT LEAD TO BAD PERFORMANCE WHEN THE CONFIGURATION TREE
IS TOO DEEP, BECAUSE OF MANY SUCCESSIVE USELESS MERGES AND SPLITS.

There are two main improvements that can be done.
  - [ ] Do not recompute trees. Even in the greedy complex case (with 10 tags)
        this algorithm finds tens of trees, so it seems that it wouldn't really scale
        for more complex root trees.
  - [ ] Instead of trying to match a specific node of the root tree with one side of
        a split, try to match any subset of nodes with one side of a split. This way
        the algorithm will generate all binary configuration trees (even the balanced
        ones), as now it only returns the longest ones it can find (especially in cases
        where in the root tree, one parent has many children).

*** TODO (Maybe) We might need to tune low level network parameters

As seen in Naiad, we might have to tune low level network parameters (or ERTS parameters)
in order to tune the system to run well with very small tcp messages between the different erlang nodes.

*** TODO Improve producer timing accuracy, by waiting for a set amount of time every time

Timestamp based producers might drift behind one another.

This problem might be solvable if we wait with send after instead of with receive after, because
then the producer will wait for a stable amount of time no matter how many messages it sends.

*** TODO Implement the possibility to have separate sink pid for each node
*** TODO Implement dynamic reconfiguration

The system at the moment chooses a configuration initially based on fixed rates. We should
have infastructure for dynamic reconfiguration, which would mean some tracing mechanism and
rerunning the compiler based on the gathered traces.

WARNING:
As a requirement for this, we have to design a snapshot
algorithm, that manages to take a snapshot of the state of the whole
system (including all the messages in flight, etc).

Snapshot algorithm:

Messages enter in the system through the routers (which are now implemented in the producer module).
Each implementation tag, arrives in exactly one node, and therefore in exactly one router.
A router thus initiate the snapshot for each of the implementation tags that it handles.

A sketch of the algorithm goes as follows:
- Each router receives a reconfigure/snapshot message
- Each router sends a message for each tag that it handles 
  (Question: To whom does it send that? To the root of the tag
   or to the whole subtree that knows about this tag??)
- After sending the snapshot message, each router stops sending
  normal messages (and heartbeats) until it is receives a
  continuation message.
- Each node receives a reconfigure message, saving that
  it has seen a reconfigure message and thus is waiting
  to get all snapshot messages to exit.
- Each node keeps processing messages until they process
  all necessary snapshot messages (Question: For which tags
  should each node wait?). When a tree node processes all the
  snapshot messages, its erlang_mailbox should be empty
  and now it can send its state (both the mailbox and 
  the processor need to send their state) to the root of the
  tree and consequently to an external master node that will create the 
  new configuration (or immediately to the master node).
- After sending those messages, each node exits gracefully.
- The master node merges the states into a consistent total state,
  and then initializes the nodes using the correctly split
  up state.
- The master node then sends a continue message to the routers/producers
  in each node.

The above can be split up in the following tasks:
  - [ ] Router
    + [ ] The router must handle a reconfigure/continue message.
    + [ ] Each node should have a router/producer for each tag so that there is no bottleneck.
    + [ ] Is it possible to have the router/producer in every node have the same name?
    + [ ] Each router sends a snapshot message to every node (TODO: Find out whether it sends it
          at the root node or all nodes in the subtree. 
  - [ ] Node
    + [ ] Extend a mailbox to be initialized with a buffer.
    + [ ] When a mailbox receives a reconfigure message, it saves it in its state
          so that it knows to wait for all snapshot messages (one for each tag).
    + [ ] When a mailbox receives all snapshot tags, it informs its processing node to
          send it its state (using a message that is handled similarly to a merge request)
    + [ ] Then the mailbox sends its state and its processing node's state to the master,
          configuration node. (otherwise it sends it to its parent, and the parent does
          the merging of the mailbox and the processor state)
  - [ ] Master/Configuration Creator
    + [ ] The master node merges the mailbox and processing nodes state (or receives them merged from the
          root node), and then reinitializes the configuration tree with the new splitted states.

*** TODO Warn if no setup_tree found and return a sequential approximation instead of failing

Instead of failing when not having found a temp_setup_tree, implement the "most parallel one".
That is, if some phase of the algorithm (for some subtree) returns no tree, then approximate 
it by returning the sequential setup tree for this tree.

*** TODO Implement the DP algorithm that maps a root tree to physical nodes
*** TODO Improve the greedy dependency graph separation algorithm

At the very least make it try to add back tags after having removed them to separate some components.

*** TODO Change the digraph library with some other graph library

Maybe aggelgian's??

*** TODO Move the specification and topology interfaces to different files

Also make make both the specification and topology be records and not tuples

*** TODO (Maybe Optimization) Unify all mailboxes in each node in one big mailbox

I am not sure how much of an optimization that is.

*** TODO (Maybe) Unify the mailbox and processing node into one Erlang process

At the moment the mailbox and the processing node are separate processes
but that doesn't really make sense. Performance wise however it is not clear
whether unifying them will improve performance at all.
Below are some pros and cons:

Pros:
+ It adds a performance overhead as every message has to be sent twice
  (even though the second message is local, it is still unnecassary)

+ Every message should be processed after it is released so it doesn't really
  make any sense putting it in the mailbox queue of the processing node.

+ It is a bit confusing having two pids for each processing node, one for its
  mailbox and one for the processing node.

Cons:
- IMPORTANT:
  When a process sends a merge request, its mailbox can still process 
  and reorder messages and release them to be ready for processing.
  Merging them both would require some different design so that merging 
  doesn't really block, and so that respones to the merge messages
  (state messages) are also handled by the mailbox immediately.

*** TODO Implement infastructure for producers.   

Their input should be a list of messages. 
The following should be configurable:

  - [-] The rate at which they send messages
    - [X] Data agnostic constant rate
    - [ ] Rate that is relative to the timestamps of the messages
  - [ ] The density of heartbeats that they will interleave in the data
  - [ ] Whether or not to reorder data that are independent

Also the node that they spawn in must be configurable

*** TODO Implement an example with a key value store and write read incr for each key
*** TODO Implement a reset feature

Instead of splitting the new state, it might be the case that the state can just be reset after the update,
thus sending one less message for every merge.

*** TODO Implement the possibility of each state type having its own dependency relation

NOTE: This just seems a part of the compiler, which should never call the splits
with wrong predicates.

The problem is that having only one state type, makes writting split and merge functions
very difficult, as one has to take into account all possible split subsets of tags. 

For that reason, we can extend each state type to have its own dependency relation, 
(which can only be stricter than the original one) to limit the possible parallelization
in each split. 

The dependency relation of a state type is used to limit the cases that we have to take
into account when designing a split and a merge.  

I am not sure whether it matters for the mailbox of each node, or whether the mailbox of
each node can just care about the total dependency relation. Probably a mailbox should 
just take into account the original dependency relation.

*** TODO Extend the system to infer missing updates

When the updates for some tags for some state types are missing, it should be possible to
infer them by applying some state type conversion and then the given update, and then the
conversion back.

*** TODO Make tests fail even if we get more messages

At the moment tests fail only if we get less (on not equal) messages to the ones that 
we expect. We should make sure that tests fail if we get more messages than expected.

*** TODO (Maybe Problem) At the moment we cannot order two messages with the same tag and timestamp

The implementation cannot break the tie between two messages with the same tag and timestamp

*** TODO Make sure that the dependencies that each node keeps are indeed the correct ones

WARNING: MAKE SURE THAT NO ASSUMPTION ABOUT THE RELATIONS OF THE PREDICATES IN THE DEPENDENCIES AND THE
         NODES ARE NEEDED.

*** TODO Allow dependencies to be based on predicates rather than tags

At the moment there is a mismatch between tags and predicates and I need to decide on which of
both to use. If we decide to use predicates we need to think about the dependencies and how should
they be encoded in the system.

*** TODO Implement an optimization that allows for merges to happen in any order

All independent merges should be mergable in any order, (associativity, commutativity).
If we only allow them to happen in the order they were split, this might deteriorate performance.

*** TODO Maybe we need an and-merge

It seems like there should be an and-merge to be paired with the and_split because it
seems that usually an or-merge could/shpould be different than the and-merge. 

*** TODO Implement infastructure that allows for a separate msg and split predicate

At the moment the message predicate of a node is the same as its split predicate

*** TODO Improve the simplicity of implementing something in our framework

Test how easy it is implement complicated queries in our intermediate language.
Try to push its expresiveness.

*** DONE Organize state of each process to be in a record

Also the data from data structures such as the configuration tree should be gotten via getter 
functions and never explicitly. Otherwise it is difficult to add and remove fields.

  - [X] node.erl
  - [X] mailbox.erl

*** DONE Move mailbox to its own file
*** DONE Separate timestamp and node id from the message, as it is never needed for the computation


WARNING: Make sure that the similarity should be for both tags and keys tags in the algorithm

Internal messages will now contain implementation tags, so at last it will be clear
what is a specification tag and what is an implementation tag

It is a little bit unclear which predicates should be for the implementation and which
should be for the specification.

  - [X] Splits/Merges predicates
  - [X] Update type definitions
  - [X] Update code in src
    + [X] node.erl
    + [X] producer.erl
    + [X] configuration.erl
    + [X] optimizer_greedy.erl
    + [X] optimizer_sequential.erl
    + [X] logger.erl
    + [X] router.erl
    + [X] anything else?
  - [X] Update code in examples
    + [X] abexample
    + [X] taxiexample
    + [X] smart_home_example

*** DONE Optimization: Reduce the redundancy of the returned setup trees

Implement a simple similarty relation as a starting point. This doesn't
necessarily need to be as coarse as possible (it wont) but it should be sound in the sense
that it doesnt group root trees as similar, when matching one of them could lead to 
different configuration trees that when matching the other.

*** DONE Implement a checkpoint mechanism

The top node (and possibly other nodes) should log the state of the system 
every time it merges (as then we have a consistent system snapshot).

  - [X] Extend the configuration generator to accept the specification, the topology,
        and a list of options. The options will override the default values in 
        an option/configuration record for the configuration generator.
  - [X] Add an option {checkpoint, Fun}, that runs the function Fun on the top
        node every time it merges the whole tree. The reason of calling a function
        is that we can take the checkpoint given an arbitrary predicate on the state.
  - [X] Implement a checkpoint function that keeps a checkpoint every time a merge
        happens by writting it to a file. The checkpoint should also contain the
        timestamp of the latest processed message.

*** DONE Implement a plot script that plots from latency logs
*** DONE Implement producers that can generate messages and timestamp them.

The current timestamp based producers lag behind of one another. The a ones,
lag behind the b one in the ab example. This leads to increasingly high latency, 
as message timestamps dont correspond to real timestamps.

Implement generator routers, that timestamp messages before sending them. 
This way (assuming that timestamp clocks are synchronized) producers wont 
drift and messages will arrive with correct timestamps. Be careful though, these
producers will lead to different results every time, as their timestamps depend on 
scheduling. Therefore they should be only used for latency/throughput measurements.

*** DONE Synchronize producers when they start executing.

Make the producers wait for a message so that they all start together.

*** DONE Implement a producer that produces events in a rate that is similar to their timestamps

This producer should produce events in times that are relative with the event timestamps.
This way, latency (and probably throughput) measurements will be closer to reality,
as latency for a message (b,1000) will start counting on 1000 and not whenever it arrives on the node
with a steady rate producer (which might be much earlier, therefore increasing the latency for b)

*** DONE Implement a tracing mechanism that gathers the statistics that we want

Latency:

Every output message is triggered by an input message. Latency can be defined as the difference between
the output time of the output message and the input time of the input message. Is this reasonable?

It can be measured if we get a timestamp before a message is routed to the processing node,
and just before its output is sent out by the sink. For this to make sense, the two timestamps
should probably be taken on the same machine (so that there is no clock drift) and there has to
be a way to associate the two messages, so probably the output should contain the tag and timestamp
of the input message that triggered the event.

We have to make sure that any latency measurement that we do is done on a system with a high enough throughput 
because otherwise we could just process everything sequentially and thus reduce latency. That is why,
we have to vary the load, or fix it to a high enough value when measuring latency, so that we have
to actually parallelize in order for the system to not choke.

*WARNING:*
In order to measure latency and throughput adequately, producers have to 
produce events in a rate that is similar to the event timestamps. Otherwise
a (b,1000) might arrive together with an (a,10) and so its latency will start counting from there.

Implementation:
  - [X] Implement a router that can be initialized to log some(/their) messages (in the abexample only bs)
  - [X] Implement a sink that can be initialized to log some output messages (in the example only sum)
  - [X] There are two ways to do logging:
    + [X] (NO) Logging could be sending log messages to some logger process.
          The logger process should be similar to the sink process (and reside in the master node)
	  and in the end of the application do some external interaction to produce the logs in a file
	  on the host.
    + [X] (Preferable) Logging could be creating a file in each container. After that, containers die, 
          but their folders are shared with the host. Then a script could gather all the log files 
	  on a log folder, and then we can analyze them,

Throughput:

This can be measured by measuring how many messages have been processed every some seconds/milliseconds.

Implementation:
  - [X] Every worker node (if initialized with log number of messages) keeps at its state how many messages
        it has processed.
  - [X] Extend workers/mailboxes to accept a get_number_messages message. When a mailbox receives this message
        it immediately sends it to the worker node.
  - [X] When a worker receives it, it sends its number of messages to the pid that asked it (and zeroes
        out its number of messages).
  - [X] A specific throughput logger process asks every mailbox in the configuration for its number of messages
        until now every some time. Then it sums them all and logs them on a file. This file can be 
        processed to find the throughput of the system.

*WARNING:* My only concern is that with these loggers, the implementation gets dirty with logging,
           messing up the logic. Is there any way to disentangle it from the worker/mailbox logic?

*** DONE Implement a complete root tree to setup tree function

At the moment the root tree to setup tree function greedily tries to
find splits that can handle any child of the root tree. 

This can obviously lead the procedure into a stuck state that
no split can be chosen, but in case of backtracking a split could have been chosen 
previously to allow us to make this setup tree.

   - [X] Make the root to setup tree complete, in the sense that it should
         return all possible splits. This way if there *IS* a way to split
         as much as the root tree requires it will find it.
   - [ ] Implement some warning message mechanism that warns the user if
         a split is missing and it is not possible to completely split
         a root tree. If it is not possible, just end up with a sequential 
	 (approximation) subtree.

Maybe implement it by passing a continuation or sth for each possible tree.
Then return a set of possible trees instead of one tree.

*** DONE Implement rates to be connected to nodes instead of processes

At the moment, rates are given for processes in nodes, rather than for nodes.

  - [X] Create a producer for each tag, and then given the configuration tree,
        decide to which process, each producer sends the data to.
  - [X] Make the configuration generator create names for processes on its own.
    + [X] Make the setup tree not contain process names anymore
    + [X] Make the configuration generator create names for nodes in some way
  - [X] Generalize rates to talk about nodes
  - [X] Implement a generic node source/producer, that receives/sends all the 
        messages that are supposed to arrive at a specific node.
  - [X] Make the optimizer tag nodes in the root tree with a node and not a process name. 

*** DONE Implement a greedy optimization strateyg
    
  - [X] Implement the most basic greedy optimization
  - [ ] To test this, try an abexample that has a lot of different a tags as well
        as a smart home example with many a tags.

Remove a tag, if it disconnects, split and iterate.

For now the greedy algorithm, assumes that there is only one split for each tiple

*** DONE Implement a configuration generator

Start implementing a trivial configuration generator
  - [X] Implement a trivial sequential optimization strategy module
    + [X] Modify abexample to use it
    + [X] Modify taxiexample to use it
    + [X] Modify smart home example to use it
  - [X] Modify SinkPid to be mailbox type and not pid
    + [X] Modify this in all examples
  - [X] Move the type definitions in the type definitions file
        (Or make sure that  can make predicates by impl tags)
  - [X] Give the optimizer to the configuration generator as argument


That given the topology and the specification of the computation,
distributes the computation accordingly (as we have done now in the 
distributed() function in the abexample)

*** DONE Implement the infastracture to distribute computation to multiple erlang nodes

Modify the implementation so that the mailbox is defined by its name and node instead from its pid.

*** DONE Optimize buffer insertions by implementing each tag buffer list as a FIFO queue

At the moment the buffer is implemented as a map of lists. 
Each message removal is optimized to take O(|Σimpl|) time as we only look the first elements of each list.

However insertions search from the beginning of each list to insert a message which is not optimal.
In theory, with the newest changes, because channels are ordered, we can never receive a message that
has an earlier timestamp than whatever message we have in this message's tag buffer. Because of that,
we can always (safely i think) add it to the end of the list.

However, with the current list implementation this takes time proportional to the number of
same tag messages in the buffer. In order to optimize this, we need to implement the list as a
real FIFO queue, where both insertions in the end, and removals from the beginning take constant time.

*** DONE Instead of sending merge requests from parent nodes, send them immediately from the input

In theory this way the input initiates all the merge requests and the nodes just enter the merging mode
when they are processing a merge.

There might be a synchronization problem, because now the merge procedures are started asynchronously

*** DONE Make sure that each input stream is ordered

So messages are also heartbeats in the sense that they update the timers. That is, heartbeats
appear only in periods of lack of messages to speed up progress.

NOTE: Before implementing that, make sure that we have decided on what the model looks like exactly

*** DONE Make sure that the top nodes propagate heartbeats to children nodes
    
WE HAVE MADE TO THE ASSUMPTION THAT EACH TAG HAS ONE ROOT NODE AND NOT MORE

DONE: This has been implemented.

In order to not block for very long periods of time. At the moment the children nodes only get the 
merge requests from upper nodes. This shouldn't really change the receiver mailbox implementation,
but only the heartbeat routing.

Before doing this, make sure that the merge requests and the heartbeats arrive in the correct order

*** DONE Create some unit testing infastructure

Create a testing framework that expects some specific output for each specific input, and in order to do
that I have to make my own sink function that will compare whatever it receives to a sample output.

In theory I have to make sure that I reorder messages that arrive from different nodes, so
if its possible I have to make sure that all outputs with reorderings (when the messages arrive from
different nodes are equial). For now I can just execute each test 100 times.

*** DONE Ensure that the assumption that children preds are subsets of the parent pred is reasonable

There is an implicit assumption that I have made that preds of children are subsets of the parent pred.
I have to make sure that it is reasonable and correct.

*** DONE Implement the buffer and its operations in a more efficient manner
    
Implemented Solution:

In order to release a message two different conditions have to be satisfied.
- It should be released after any message that is dependent to it and has an earlier timestamp
- It should be released after we are sure that we have received all those messages with an
  earlier timestamp.

In our buffer we have at any point for each tag σ:
- A (possibly empty) sequence of messages that is ordered by timestamp. Its first message is the
earliest message of tag σ that the mailbox hasn't still released.
- A timer that indicates the largest timestamp that the mailbox has seen for this tag.

Checking whether a message can be released:
To release a message with tag σ' we have to make sure that for each of its dependencies σ'', 
its timestamp is smaller than both the timer for σ'' and the earliest message for σ''.

Whenever the mailbox gets a new heartbeat it:
1. Updates the timers for this tag
2. Checks whether any message in the buffer can be released based on the new timer values

Whenever the mailbox gets a new message, it:
1. The message is added to the ordered queue with messages of the same tag,
   as the earliest messages of the same tag can be released first
   (this doesn't mean that they should, by they almost always will be)
2. Updates the timers for the tag
3. Checks whether the new message can be released

There is a problem however, releasing a message can create an arbitrary cascade of new
releases on the dependencies of this specific message. It doesn't really matter though.

ALTERNATIVE: Or as a priority queue

Instead of sorting everything in the buffer and then traversing it every time to clear messages,
we might be able to implement it as a dependency DAG, where the source messages block the ones that
are after them from being released. 

Then, each time we want to clear the buffer we will only look at the sources, and only if we do
release one of them, we will look at its next messages.

Each time we want to add a message, we find the latest dependent messages to it in the DAG, and we 
insert the new message after them (together with edges from them to it).

*** DONE BUG: Heatbeats releases all messages, not caring about the messages that they depend on

At the moment, after every heartbeat, every message that has all its dependent timers higher than it,
is released. However that is not correct, because there might be a message that they depend on,
that depends on more tags, that was received before, but hasn't been released. This leads to inconsistencies.

FIX: 
Implement the clear buffer to only clear all the messages sequentially until it finds one which cannot
be cleared. This is a naive way to solve this bug, as this way messages might have to wait in the buffer
fo messages that they do not depend on to be released. Ideally an implementation would only release a message
if there is no message that it depends on previously in the buffer.

*** DONE Optimize the clear_buffer function

After the above bug fix, messages wait in the buffer for every message that has a smaller timestamp
to be released first. However, this can lead to a situation where messages wait in the buffer despite
being independent than anything else before them. 

An improvement (that is still naive however as it traverses the buffer every time it needs to clear) is
to sequentially traverse the buffer, and keep the first timestamp of each tag that we see. This way
we we only release messages that don't have a dependent tag that has arrived earlier than them but hasn't
been released.

*** DONE Implement a taxi example where {id,1} is dependent to itself but not to {id,2}
    - [X] Define the computation
    - [X] Implement a producer that create {x,y} line coordinates for each taxi
    - [X] Define a sequential configuration
    - [X] Define a distributed configuration

This could be messages with the position of the taxi, that arrive every second, and we want
to get the distance that the taxi has covered in every hour. So we need to compute the distance
between every two *consecutive* points and add them together.

NOTE: Before finishing this, I have to make sure that the bug below is solved.

*** DONE Handle a merge message as both a heartbeat and a normal message
    - [X] Add the merge message to the buffer, and then clear the buffer using it as a heartbeat
    - [X] Make sure that the dependencies of the merge message are handled correctly
      + [X] Handle merge req dependencies correctly 1.1
      + [X] Send merge messages as a parent asynchronously and then wait for both 1.2
    - [X] After this bug is solved, test every example until now, to ensure correctness
    - [X] Remove the unused functions in node.erl
    - [X] Move the configuration tree functions from node.erl in the configuration.erl


Solution:
First add the merge to the buffer, and then clear the buffer (using the merge as a heartbeat).

The way it is done now, a merge messafe clears the buffer, but is then sent immediately to the node,
which could lead to a bug. Example: An "a" mailbox hasn't received an a heartbeat but it receives a 
"b" merge request. This will lead to the merge request being forwarded to the node, before the "a"s
that should have been already processed.

Problem1:
In order to implement this solution, I have to make sure that the merge message will be handled correctly,
and cleared at the next a-heartbeat (or even immediately). Because of this, I might need to revise the 
clear dependencies functions that I call befoee initializing the mailbox to not delete the keys that
are not in a node's predicate. 

Problem1.1:
At the moment node 1 doesn't get the id,2 messages or heartbeats, so it is impossible for it to clear 
the merge message. It might be solvable in the following way. Instead of only removing the dependencies
of my children, I should remove the dependencies of every node, that is not my father (or grandfather...).
In theory, I will never learn about my children's heartbeats because I will ask with a merge, and I will
always learn from my parents (father, grandfather...). This constitutes my alpha mapping, that is
all the tags except the ones that my children and my cousins, siblings, uncles ... deal with. However,
I need to be careful because I might remove my own predicate like this. In reality I have to only add myself and
all my parents predicate after removing their other childrens. So add Mine, (Father - OtherChild), 
(Grandfather - OtherChild(Uncle))... 

PROBLEM1.1: I have implemented this but it still has a problem on the first run, it sometimes
            returns 59 and 58 and sometimes it returns 58 and 58.

WARNING: MAKE SURE THAT NO ASSUMPTION ABOUT THE RELATIONS OF THE PREDICATES IN THE DEPENDENCIES AND THE
         NODES ARE NEEDED.

Problem1.2:
Also, a parent doesn't asynchronously send the merge messages but it rather blocks on each child,
which is wrong. It should block for both children together

*** DONE Optimize the add message to buffer to not wait for the next heartbeat

At the moment, a message is added to the buffer without even thinking whether it might need to be released
or not. Think of a way to optimize this so that a new message is not necessarily added to the buffer, 
but could rather be sent to the node (before or after other messages that might also need to be sent)

Maybe:
This optimization might correlate with the clear_buffer optimization that is described above. If we 
add a new message in the buffer, in an earlier position than any of its dependencies, and its dependent
timers are already higher than it, then we can release it immediately


This degrades performance as some messages might not need to be ordered in the buffer. This way
we sort everything no matter whether they do need to be ordered or not.

*** DONE Improve the mailbox to only forward heartbeats to nodes for which it satisfies their pred

In order for this to work, higher nodes should just ask the lower ones with their merges when they need.
In order for that to happen, we need to read (or be able to compute) the alpha mapping from the beta mapping
that we currently have as a predicate. It is important that the predicates are set up correctly in the beginning.

In essence, a parent node, loses messages that satisfy its descendant predicates, 
and so it shouldn't receive heartbeats for those messages, as it will learn from them
when asking for a merge.

*** DONE Implement an optimization that allows for part of the state to be left behind in a merge

This can be implemented as an or-split, that has an empty predicate where the part of the state is left
behind.

*** DONE Implement a message tracing mechanism

It should trace all the messages that are exchanged, and the function calls that are made.
Then by using this information together with the topology of the network and a mapping
of the process ids to nodes, we could estimate statistics on the execution of the program.

*** DONE (Make sure that the implementation makes sense) Implement the alpha and beta mappings

The alpha mapping used to be what messages must a node receive in order to be able to process
the messages in its beta mapping.

However, it seems like thsi can be derived from the dependencies and the beta mapping (which currenty is
a boolean predicate on messages). 

The alphia mapping of a node, is the dependencies that it waits on, and the process to derive it
is described in node:remove_unnecassary_dependencies/3. In short, a node doesn't need to wait
for the messages that are processed by its descendants because it will learn for them when it asks
for a merge, as only the leaf nodes do processing without merging.

*** DONE Move the implementation source in ./src and the examples in ./examples

** Work TODO

*** TODO Implement a configuration generator

Start implementing a trivial configuration generator
  - [ ] Implement a trivial sequential optimization strategy module
    + [ ] Modify abexample to use it
    + [ ] Modify taxiexample to use it
    + [ ] Modify smart home example to use it
  - [X] Modify SinkPid to be mailbox type and not pid
    + [X] Modify this in all examples
  - [ ] Make both the specification and topology be records and not tuples
  - [ ] Move the specification and topology interfaces to different files
  - [ ] Move the type definitions in the type definitions file
  - [ ] Modify Splits/Merges/Nodes to not take predicates, but Implementation tags
        (Or make sure that  can make predicates by impl tags)


That given the topology and the specification of the computation,
distributes the computation accordingly (as we have done now in the 
distributed() function in the abexample)

*** TODO Implement the possibility to have separate sink pid for each node
*** TODO Implement a greedy optimization strategy
*** TODO Implement infastructure for producers.   

Their input should be a list of messages. 
The following should be configurable:

  - [-] The rate at which they send messages
    - [X] Data agnostic constant rate
    - [ ] Rate that is relative to the timestamps of the messages
  - [ ] The density of heartbeats that they will interleave in the data
  - [ ] Whether or not to reorder data that are independent

Also the node that they spawn in must be configurable

*** TODO Implement an example with a key value store and write read incr for each key
*** TODO Implement a reset feature

Instead of splitting the new state, it might be the case that the state can just be reset after the update,
thus sending one less message for every merge.

*** TODO Implement the possibility of each state type having its own dependency relation

NOTE: This just seems a part of the compiler, which should never call the splits
with wrong predicates.

The problem is that having only one state type, makes writting split and merge functions
very difficult, as one has to take into account all possible split subsets of tags. 

For that reason, we can extend each state type to have its own dependency relation, 
(which can only be stricter than the original one) to limit the possible parallelization
in each split. 

The dependency relation of a state type is used to limit the cases that we have to take
into account when designing a split and a merge.  

I am not sure whether it matters for the mailbox of each node, or whether the mailbox of
each node can just care about the total dependency relation. Probably a mailbox should 
just take into account the original dependency relation.

*** TODO Implement dynamic reconfiguration

The system at the moment chooses a configuration initially based on fixed rates. We should
have infastructure for dynamic reconfiguration, which would mean some tracing mechanism and
rerunning the compiler based on the gathered traces.

*** TODO Extend the system to infer missing updates

When the updates for some tags for some state types are missing, it should be possible to
infer them by applying some state type conversion and then the given update, and then the
conversion back.

*** TODO Make tests fail even if we get more messages

At the moment tests fail only if we get less (on not equal) messages to the ones that 
we expect. We should make sure that tests fail if we get more messages than expected.

*** TODO (Maybe Problem) At the moment we cannot order two messages with the same tag and timestamp

The implementation cannot break the tie between two messages with the same tag and timestamp

*** TODO Make sure that the dependencies that each node keeps are indeed the correct ones

WARNING: MAKE SURE THAT NO ASSUMPTION ABOUT THE RELATIONS OF THE PREDICATES IN THE DEPENDENCIES AND THE
         NODES ARE NEEDED.

*** TODO Allow dependencies to be based on predicates rather than tags

At the moment there is a mismatch between tags and predicates and I need to decide on which of
both to use. If we decide to use predicates we need to think about the dependencies and how should
they be encoded in the system.

*** TODO Implement an optimization that allows for merges to happen in any order

All independent merges should be mergable in any order, (associativity, commutativity).
If we only allow them to happen in the order they were split, this might deteriorate performance.

*** TODO Maybe we need an and-merge

It seems like there should be an and-merge to be paired with the and_split because it
seems that usually an or-merge could/shpould be different than the and-merge. 

*** TODO Implement infastructure that allows for a separate msg and split predicate

At the moment the message predicate of a node is the same as its split predicate

*** TODO Improve the simplicity of implementing something in our framework

Test how easy it is implement complicated queries in our intermediate language.
Try to push its expresiveness.
*** DONE Implement the infastracture to distribute computation to multiple erlang nodes

Modify the implementation so that the mailbox is defined by its name and node instead from its pid.

*** DONE Optimize buffer insertions by implementing each tag buffer list as a FIFO queue

At the moment the buffer is implemented as a map of lists. 
Each message removal is optimized to take O(|Σimpl|) time as we only look the first elements of each list.

However insertions search from the beginning of each list to insert a message which is not optimal.
In theory, with the newest changes, because channels are ordered, we can never receive a message that
has an earlier timestamp than whatever message we have in this message's tag buffer. Because of that,
we can always (safely i think) add it to the end of the list.

However, with the current list implementation this takes time proportional to the number of
same tag messages in the buffer. In order to optimize this, we need to implement the list as a
real FIFO queue, where both insertions in the end, and removals from the beginning take constant time.

*** DONE Instead of sending merge requests from parent nodes, send them immediately from the input

In theory this way the input initiates all the merge requests and the nodes just enter the merging mode
when they are processing a merge.

There might be a synchronization problem, because now the merge procedures are started asynchronously

*** DONE Make sure that each input stream is ordered

So messages are also heartbeats in the sense that they update the timers. That is, heartbeats
appear only in periods of lack of messages to speed up progress.

NOTE: Before implementing that, make sure that we have decided on what the model looks like exactly

*** DONE Make sure that the top nodes propagate heartbeats to children nodes
    
WE HAVE MADE TO THE ASSUMPTION THAT EACH TAG HAS ONE ROOT NODE AND NOT MORE

DONE: This has been implemented.

In order to not block for very long periods of time. At the moment the children nodes only get the 
merge requests from upper nodes. This shouldn't really change the receiver mailbox implementation,
but only the heartbeat routing.

Before doing this, make sure that the merge requests and the heartbeats arrive in the correct order

*** DONE Create some unit testing infastructure

Create a testing framework that expects some specific output for each specific input, and in order to do
that I have to make my own sink function that will compare whatever it receives to a sample output.

In theory I have to make sure that I reorder messages that arrive from different nodes, so
if its possible I have to make sure that all outputs with reorderings (when the messages arrive from
different nodes are equial). For now I can just execute each test 100 times.

*** DONE Ensure that the assumption that children preds are subsets of the parent pred is reasonable

There is an implicit assumption that I have made that preds of children are subsets of the parent pred.
I have to make sure that it is reasonable and correct.

*** DONE Implement the buffer and its operations in a more efficient manner
    
Implemented Solution:

In order to release a message two different conditions have to be satisfied.
- It should be released after any message that is dependent to it and has an earlier timestamp
- It should be released after we are sure that we have received all those messages with an
  earlier timestamp.

In our buffer we have at any point for each tag σ:
- A (possibly empty) sequence of messages that is ordered by timestamp. Its first message is the
earliest message of tag σ that the mailbox hasn't still released.
- A timer that indicates the largest timestamp that the mailbox has seen for this tag.

Checking whether a message can be released:
To release a message with tag σ' we have to make sure that for each of its dependencies σ'', 
its timestamp is smaller than both the timer for σ'' and the earliest message for σ''.

Whenever the mailbox gets a new heartbeat it:
1. Updates the timers for this tag
2. Checks whether any message in the buffer can be released based on the new timer values

Whenever the mailbox gets a new message, it:
1. The message is added to the ordered queue with messages of the same tag,
   as the earliest messages of the same tag can be released first
   (this doesn't mean that they should, by they almost always will be)
2. Updates the timers for the tag
3. Checks whether the new message can be released

There is a problem however, releasing a message can create an arbitrary cascade of new
releases on the dependencies of this specific message. It doesn't really matter though.

ALTERNATIVE: Or as a priority queue

Instead of sorting everything in the buffer and then traversing it every time to clear messages,
we might be able to implement it as a dependency DAG, where the source messages block the ones that
are after them from being released. 

Then, each time we want to clear the buffer we will only look at the sources, and only if we do
release one of them, we will look at its next messages.

Each time we want to add a message, we find the latest dependent messages to it in the DAG, and we 
insert the new message after them (together with edges from them to it).

*** DONE BUG: Heatbeats releases all messages, not caring about the messages that they depend on

At the moment, after every heartbeat, every message that has all its dependent timers higher than it,
is released. However that is not correct, because there might be a message that they depend on,
that depends on more tags, that was received before, but hasn't been released. This leads to inconsistencies.

FIX: 
Implement the clear buffer to only clear all the messages sequentially until it finds one which cannot
be cleared. This is a naive way to solve this bug, as this way messages might have to wait in the buffer
fo messages that they do not depend on to be released. Ideally an implementation would only release a message
if there is no message that it depends on previously in the buffer.

*** DONE Optimize the clear_buffer function

After the above bug fix, messages wait in the buffer for every message that has a smaller timestamp
to be released first. However, this can lead to a situation where messages wait in the buffer despite
being independent than anything else before them. 

An improvement (that is still naive however as it traverses the buffer every time it needs to clear) is
to sequentially traverse the buffer, and keep the first timestamp of each tag that we see. This way
we we only release messages that don't have a dependent tag that has arrived earlier than them but hasn't
been released.

*** DONE Implement a taxi example where {id,1} is dependent to itself but not to {id,2}
    - [X] Define the computation
    - [X] Implement a producer that create {x,y} line coordinates for each taxi
    - [X] Define a sequential configuration
    - [X] Define a distributed configuration

This could be messages with the position of the taxi, that arrive every second, and we want
to get the distance that the taxi has covered in every hour. So we need to compute the distance
between every two *consecutive* points and add them together.

NOTE: Before finishing this, I have to make sure that the bug below is solved.

*** DONE Handle a merge message as both a heartbeat and a normal message
    - [X] Add the merge message to the buffer, and then clear the buffer using it as a heartbeat
    - [X] Make sure that the dependencies of the merge message are handled correctly
      + [X] Handle merge req dependencies correctly 1.1
      + [X] Send merge messages as a parent asynchronously and then wait for both 1.2
    - [X] After this bug is solved, test every example until now, to ensure correctness
    - [X] Remove the unused functions in node.erl
    - [X] Move the configuration tree functions from node.erl in the configuration.erl


Solution:
First add the merge to the buffer, and then clear the buffer (using the merge as a heartbeat).

The way it is done now, a merge messafe clears the buffer, but is then sent immediately to the node,
which could lead to a bug. Example: An "a" mailbox hasn't received an a heartbeat but it receives a 
"b" merge request. This will lead to the merge request being forwarded to the node, before the "a"s
that should have been already processed.

Problem1:
In order to implement this solution, I have to make sure that the merge message will be handled correctly,
and cleared at the next a-heartbeat (or even immediately). Because of this, I might need to revise the 
clear dependencies functions that I call befoee initializing the mailbox to not delete the keys that
are not in a node's predicate. 

Problem1.1:
At the moment node 1 doesn't get the id,2 messages or heartbeats, so it is impossible for it to clear 
the merge message. It might be solvable in the following way. Instead of only removing the dependencies
of my children, I should remove the dependencies of every node, that is not my father (or grandfather...).
In theory, I will never learn about my children's heartbeats because I will ask with a merge, and I will
always learn from my parents (father, grandfather...). This constitutes my alpha mapping, that is
all the tags except the ones that my children and my cousins, siblings, uncles ... deal with. However,
I need to be careful because I might remove my own predicate like this. In reality I have to only add myself and
all my parents predicate after removing their other childrens. So add Mine, (Father - OtherChild), 
(Grandfather - OtherChild(Uncle))... 

PROBLEM1.1: I have implemented this but it still has a problem on the first run, it sometimes
            returns 59 and 58 and sometimes it returns 58 and 58.

WARNING: MAKE SURE THAT NO ASSUMPTION ABOUT THE RELATIONS OF THE PREDICATES IN THE DEPENDENCIES AND THE
         NODES ARE NEEDED.

Problem1.2:
Also, a parent doesn't asynchronously send the merge messages but it rather blocks on each child,
which is wrong. It should block for both children together

*** DONE Optimize the add message to buffer to not wait for the next heartbeat

At the moment, a message is added to the buffer without even thinking whether it might need to be released
or not. Think of a way to optimize this so that a new message is not necessarily added to the buffer, 
but could rather be sent to the node (before or after other messages that might also need to be sent)

Maybe:
This optimization might correlate with the clear_buffer optimization that is described above. If we 
add a new message in the buffer, in an earlier position than any of its dependencies, and its dependent
timers are already higher than it, then we can release it immediately


This degrades performance as some messages might not need to be ordered in the buffer. This way
we sort everything no matter whether they do need to be ordered or not.

*** DONE Improve the mailbox to only forward heartbeats to nodes for which it satisfies their pred

In order for this to work, higher nodes should just ask the lower ones with their merges when they need.
In order for that to happen, we need to read (or be able to compute) the alpha mapping from the beta mapping
that we currently have as a predicate. It is important that the predicates are set up correctly in the beginning.

In essence, a parent node, loses messages that satisfy its descendant predicates, 
and so it shouldn't receive heartbeats for those messages, as it will learn from them
when asking for a merge.

*** DONE Implement an optimization that allows for part of the state to be left behind in a merge

This can be implemented as an or-split, that has an empty predicate where the part of the state is left
behind.

*** DONE Implement a message tracing mechanism

It should trace all the messages that are exchanged, and the function calls that are made.
Then by using this information together with the topology of the network and a mapping
of the process ids to nodes, we could estimate statistics on the execution of the program.

*** DONE (Make sure that the implementation makes sense) Implement the alpha and beta mappings

The alpha mapping used to be what messages must a node receive in order to be able to process
the messages in its beta mapping.

However, it seems like thsi can be derived from the dependencies and the beta mapping (which currenty is
a boolean predicate on messages). 

The alphia mapping of a node, is the dependencies that it waits on, and the process to derive it
is described in node:remove_unnecassary_dependencies/3. In short, a node doesn't need to wait
for the messages that are processed by its descendants because it will learn for them when it asks
for a merge, as only the leaf nodes do processing without merging.

*** DONE Move the implementation source in ./src and the examples in ./examples
